{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import losses, models, optimizers\n",
    "from keras.layers import (Input, Dense, Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten, GlobalMaxPool2D, MaxPool2D, concatenate, Activation)\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from keras import backend as K\n",
    "from keras.callbacks import (EarlyStopping, ModelCheckpoint, TensorBoard)\n",
    "from keras.activations import relu, softmax\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to True for full dataset and learning\n",
    "COMPLETE_RUN = False\n",
    "data_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path + \"/train_noisy.csv\")\n",
    "test = pd.read_csv(data_path + \"/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples= 19815   Number of classes= 1168\n",
      "Number of test examples= 1120   Number of classes= 80\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train examples=\", train.shape[0], \"  Number of classes=\", len(set(train['labels'])))\n",
    "print(\"Number of test examples=\", test.shape[0], \"  Number of classes=\", len(set(test.columns[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train['labels'])) # But now we're working with less than 7% of the original number of labels that describes more 83% of all the clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16566\n"
     ]
    }
   ],
   "source": [
    "# Same as with the curated set, we'll ignore multi labeled rows for now.\n",
    "# Unlike the curated set though this will drastically cut down a large portion of the data set\n",
    "train = train[train['labels'].isin(test.columns[1:])]\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "category_group = train.groupby(['labels']).count()\n",
    "category_group.columns = ['counts']\n",
    "print(len(category_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Configuration object stores those learning parameters that are shared\n",
    "# between data generators models, and training functions. Anything that is\n",
    "# global as far as the training is concerned can become the part of Configuration object.\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self,\n",
    "                 sampling_rate=16000,\n",
    "                 audio_duration=2, \n",
    "                 n_classes=len(category_group),\n",
    "                 use_mfcc=False,\n",
    "                 n_folds=10,\n",
    "                 learning_rate=0.0001, \n",
    "                 max_epochs=50,\n",
    "                 n_mfcc=20):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.audio_duration = audio_duration\n",
    "        self.n_classes = n_classes\n",
    "        self.use_mfcc = use_mfcc\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_folds = n_folds\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "        self.audio_length = self.sampling_rate * self.audio_duration\n",
    "        if self.use_mfcc:\n",
    "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
    "        else:\n",
    "            self.dim = (self.audio_length, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataGenerator class inherits from keras.utils.Sequence .\n",
    "# It is useful for preprocessing and feeding the data to a Keras model.\n",
    "\n",
    "# Note: Sequence are a safer way to do multiprocessing.\n",
    "# This structure guarantees that the network will only train once on each\n",
    "# sample per epoch which is not the case with generators.\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self,\n",
    "                 config,\n",
    "                 data_dir,\n",
    "                 list_IDs,\n",
    "                 labels=None, \n",
    "                 batch_size=64,\n",
    "                 preprocessing_fn=lambda x: x):\n",
    "        self.config = config\n",
    "        self.data_dir = data_dir\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.preprocessing_fn = preprocessing_fn\n",
    "        self.on_epoch_end()\n",
    "        self.dim = self.config.dim\n",
    "\n",
    "    def __len__(self):\n",
    "        # Once initialized with a batch_size, DataGenerator computes the number of batches in an epoch.\n",
    "        # The __len__ method tells Keras how many batches to draw in each epoch.\n",
    "        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # The __getitem__ method takes an index (which is the batch number) and\n",
    "        # returns a batch of the data (both X and y) after calculating the offset.\n",
    "        # During test time, only X is returned.\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        return self.__data_generation(list_IDs_temp)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # If we want to perform some action after each epoch (like shuffle the data,\n",
    "        # or increase the proportion of augmented data), we can use the on_epoch_end method.\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        cur_batch_size = len(list_IDs_temp)\n",
    "        X = np.empty((cur_batch_size, *self.dim))\n",
    "\n",
    "        input_length = self.config.audio_length\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            file_path = self.data_dir + ID\n",
    "            \n",
    "            # Read and Resample the audio\n",
    "            data, _ = librosa.core.load(file_path, sr=self.config.sampling_rate, res_type='kaiser_fast')\n",
    "\n",
    "            # Random offset / Padding\n",
    "            if len(data) > input_length:\n",
    "                max_offset = len(data) - input_length\n",
    "                offset = np.random.randint(max_offset)\n",
    "                data = data[offset:(input_length+offset)]\n",
    "            else:\n",
    "                if input_length > len(data):\n",
    "                    max_offset = input_length - len(data)\n",
    "                    offset = np.random.randint(max_offset)\n",
    "                else:\n",
    "                    offset = 0\n",
    "                data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
    "                \n",
    "            # Normalization + Other Preprocessing\n",
    "            if self.config.use_mfcc:\n",
    "                data = librosa.feature.mfcc(data, sr=self.config.sampling_rate, n_mfcc=self.config.n_mfcc)\n",
    "                data = np.expand_dims(data, axis=-1)\n",
    "            else:\n",
    "                data = self.preprocessing_fn(data)[:, np.newaxis]\n",
    "            print(X, data)\n",
    "            X[i,] = data\n",
    "\n",
    "        if self.labels is not None:\n",
    "            y = np.empty(cur_batch_size, dtype=int)\n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                y[i] = self.labels[ID]\n",
    "            return X, to_categorical(y, num_classes=self.config.n_classes)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_dummy_model(config):\n",
    "    \n",
    "    nclass = config.n_classes\n",
    "    \n",
    "    inp = Input(shape=(config.dim[0],config.dim[1],1))\n",
    "    x = GlobalMaxPool2D()(inp)\n",
    "    out = Dense(nclass, activation=softmax)(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    opt = optimizers.Adam(config.learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_2d_conv_model(config):\n",
    "    \n",
    "    nclass = config.n_classes\n",
    "    \n",
    "    inp = Input(shape=(config.dim[0],config.dim[1],1))\n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "    \n",
    "    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPool2D()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    out = Dense(nclass, activation=softmax)(x)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=out)\n",
    "    opt = optimizers.Adam(config.learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to convert raw labels to integer indices\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = list(train['labels'].unique())\n",
    "label_idx = {label: i for i, label in enumerate(LABELS)}\n",
    "\n",
    "train.set_index(\"fname\", inplace=True)\n",
    "test.set_index(\"fname\", inplace=True)\n",
    "\n",
    "train[\"label_idx\"] = train['labels'].apply(lambda x: label_idx[x])\n",
    "\n",
    "if not COMPLETE_RUN:\n",
    "    train = train[:2000]\n",
    "    test = test[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw labels with integer indices\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(sampling_rate=44100, audio_duration=2, n_folds=10, \n",
    "                learning_rate=0.001, use_mfcc=True, n_mfcc=40)\n",
    "if not COMPLETE_RUN:\n",
    "    config = Config(sampling_rate=44100, audio_duration=2, n_folds=2, \n",
    "                    max_epochs=1, use_mfcc=True, n_mfcc=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, config, data_dir):\n",
    "    X = np.empty(shape=(df.shape[0], config.dim[0], config.dim[1], 1))\n",
    "    input_length = config.audio_length\n",
    "    for i, fname in enumerate(df.index):\n",
    "        file_path = data_dir + fname\n",
    "        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, res_type=\"kaiser_fast\")\n",
    "\n",
    "        # Random offset / Padding\n",
    "        if len(data) > input_length:\n",
    "            max_offset = len(data) - input_length\n",
    "            offset = np.random.randint(max_offset)\n",
    "            data = data[offset:(input_length+offset)]\n",
    "        else:\n",
    "            if input_length > len(data):\n",
    "                max_offset = input_length - len(data)\n",
    "                offset = np.random.randint(max_offset)\n",
    "            else:\n",
    "                offset = 0\n",
    "            data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
    "\n",
    "        data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)\n",
    "        data = np.expand_dims(data, axis=-1)\n",
    "        X[i,] = data\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "X_train = prepare_data(train, config, data_path + '/train_noisy/')\n",
    "X_test = prepare_data(test, config, data_path + '/test/')\n",
    "y_train = to_categorical(train.label_idx.astype('str'), num_classes=config.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training 2D Conv on MFCC\n",
    "\n",
    "PREDICTION_FOLDER = \"predictions_2d_conv\"\n",
    "if not os.path.exists(PREDICTION_FOLDER):\n",
    "    os.mkdir(PREDICTION_FOLDER)\n",
    "if os.path.exists('logs/' + PREDICTION_FOLDER):\n",
    "    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=config.n_folds)\n",
    "\n",
    "for i, (train_split, val_split) in enumerate(skf.split(train.index, train.label_idx)):\n",
    "    K.clear_session()\n",
    "    X, y, X_val, y_val = X_train[train_split], y_train[train_split], X_train[val_split], y_train[val_split]\n",
    "    checkpoint = ModelCheckpoint('best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%i'%i, write_graph=True)\n",
    "    callbacks_list = [checkpoint, early, tb]\n",
    "    print(\"#\"*50)\n",
    "    print(\"Fold: \", i)\n",
    "    model = get_2d_conv_model(config)\n",
    "    history = model.fit(X, y, validation_data=(X_val, y_val), callbacks=callbacks_list, \n",
    "                        batch_size=64, epochs=config.max_epochs)\n",
    "    model.load_weights('best_%d.h5'%i)\n",
    "\n",
    "    # Save train predictions\n",
    "    predictions = model.predict(X_train, batch_size=64, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Save test predictions\n",
    "    predictions = model.predict(X_test, batch_size=64, verbose=1)\n",
    "    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n",
    "\n",
    "    # Make a submission file\n",
    "    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n",
    "    predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "    test['label'] = predicted_labels\n",
    "    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembling 2D Conv Predictions¶\n",
    "\n",
    "pred_list = []\n",
    "for i in range(config.n_folds):\n",
    "    pred_list.append(np.load(\"./predictions_2d_conv/test_predictions_%d.npy\"%i))\n",
    "prediction = np.ones_like(pred_list[0])\n",
    "for pred in pred_list:\n",
    "    prediction = prediction*pred\n",
    "prediction = prediction**(1./len(pred_list))\n",
    "# Make a submission file\n",
    "top_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\n",
    "predicted_labels = [' '.join(list(x)) for x in top_3]\n",
    "test = pd.read_csv(data_path + '/sample_submission.csv')\n",
    "test['label'] = predicted_labels\n",
    "test[['fname', 'label']].to_csv(\"2d_conv_ensembled_noisy_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
